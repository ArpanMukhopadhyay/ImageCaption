{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner(nn.Module):\n",
    "    def __init__(self, context_length, vocab_size, num_blocks, model_dim, num_heads, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.cnn_encoder = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        test_image = torch.zeros(1,3,224,224)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cnn_output = self.cnn_encoder(test_image)\n",
    "        in_features = cnn_output.shape[1]    \n",
    "        self.project = nn.Linear(in_features, model_dim)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embeddings = nn.Embedding(context_length, model_dim)\n",
    "\n",
    "        block = nn.TransformerDecoderLayer(model_dim, num_heads, 2*model_dim, dropout=dropout_prob, batch_first=True, norm_first =True)\n",
    "        self.blocks = nn.TransformerDecoderLayer(block, num_blocks)\n",
    "\n",
    "        self.vocab_projection = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, images, true_labels):\n",
    "        tok_embedded = self.word_embeddings(true_labels)\n",
    "        B,T = true_labels.shape\n",
    "        positions = torch.arange(T).to(device)\n",
    "        pos_embedded = self.pos_embeddings(positions)\n",
    "        total_emebddings = tok_embedded + pos_embedded #input to blocks\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded_image = self.project = (self.cnn_encoder(images).view(B,-1))\n",
    "        \n",
    "        img_for_attention = torch.unsqueeze(encoded_image, 1)\n",
    "\n",
    "        #Causal/Subsequent Mask\n",
    "        attention_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n",
    "        block_output = self.blocks(total_emebddings, img_for_attention, tgt_mask=attention_mask)\n",
    "\n",
    "        vocabulary_vector = self.vocab_projection(block_output) #B,T,V\n",
    "\n",
    "        return vocabulary_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_filename = 'Flickr8k/captions.txt'\n",
    "missing = '2258277193_586949ec62'\n",
    "\n",
    "with open(caption_filename) as captions:\n",
    "    lines = captions.readlines()\n",
    "\n",
    "get_captions = {}\n",
    "all_captions = []\n",
    "\n",
    "for i in range(1,len(lines)):\n",
    "    data = lines[i].rstrip('\\n').split('.jpg,')\n",
    "    img_name = data[0] + '.jpg'\n",
    "    if img_name == missing:\n",
    "        continue\n",
    "\n",
    "    caption_list = get_captions.get(img_name, [])\n",
    "    caption_list.append(data[1])\n",
    "    get_captions[img_name] = caption_list\n",
    "    all_captions.append(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40455\n"
     ]
    }
   ],
   "source": [
    "print(len(all_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Dataframe(columns=['filename', 'caption'])\n",
    "df['filename'] = get_captions.keys()\n",
    "df['caption'] = df['filename'].map(lambda filename: get_captions(filename))\n",
    "\n",
    "vocab_frequency = Counter()\n",
    "word_tokeniser = get_tokenizer('basic english')\n",
    "\n",
    "for c in all_captions:\n",
    "    vocab_frequency.update(word_tokeniser(c))\n",
    "\n",
    "vocabulary_mapping = torchtext.vocab.vocab(vocab_frequency)\n",
    "vocabulary_mapping.insert_token('<UNKNOWN>',0)\n",
    "vocabulary_mapping.insert_token('<PAD>',1)\n",
    "vocabulary_mapping.insert_token('<START>',2)\n",
    "vocabulary_mapping.insert_token('<END>',3)\n",
    "vocabulary_mapping.set_default_index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 20\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, captions = self.df.iloc[idx]\n",
    "        encoded_captions = []\n",
    "        for i,cap in enumerate(captions):\n",
    "            splitted = word_tokeniser(cap)\n",
    "\n",
    "            integers = [vocabulary_mapping[word] for word in splitted]\n",
    "            integers = [2] + integers + [3]\n",
    "\n",
    "            if len(integers) <= context_length:\n",
    "                pads_to_add = context_length - len(integers)\n",
    "                integers += [1] * pads_to_add\n",
    "            else:\n",
    "                integers = integers[:context_length - 1] + [3]\n",
    "            \n",
    "            encoded_captions.append(torch.tensor(integers, dtype=torch.long))\n",
    "\n",
    "        random_idx = torch.randint(len(encoded_captions), (1,)).item()\n",
    "        return image, encoded_captions[random_idx]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
