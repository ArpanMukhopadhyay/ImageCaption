{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner(nn.Module):\n",
    "    def __init__(self, context_length, vocab_size, num_blocks, model_dim, num_heads, dropout_prob):\n",
    "        super().__init__()\n",
    "        self.cnn_encoder = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "        test_image = torch.zeros(1,3,224,224)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            cnn_output = self.cnn_encoder(test_image)\n",
    "        in_features = cnn_output.shape[1]    \n",
    "        self.project = nn.Linear(in_features, model_dim)\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, model_dim)\n",
    "        self.pos_embeddings = nn.Embedding(context_length, model_dim)\n",
    "\n",
    "        block = nn.TransformerDecoderLayer(model_dim, num_heads, 2*model_dim, dropout=dropout_prob, batch_first=True, norm_first =True)\n",
    "        self.blocks = nn.TransformerDecoderLayer(block, num_blocks)\n",
    "\n",
    "        self.vocab_projection = nn.Linear(model_dim, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, images, true_labels):\n",
    "        tok_embedded = self.word_embeddings(true_labels)\n",
    "        B,T = true_labels.shape\n",
    "        positions = torch.arange(T).to(device)\n",
    "        pos_embedded = self.pos_embeddings(positions)\n",
    "        total_emebddings = tok_embedded + pos_embedded #input to blocks\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded_image = self.project = (self.cnn_encoder(images).view(B,-1))\n",
    "        \n",
    "        img_for_attention = torch.unsqueeze(encoded_image, 1)\n",
    "\n",
    "        #Causal/Subsequent Mask\n",
    "        attention_mask = nn.Transformer.generate_square_subsequent_mask(T).to(device)\n",
    "        block_output = self.blocks(total_emebddings, img_for_attention, tgt_mask=attention_mask)\n",
    "\n",
    "        vocabulary_vector = self.vocab_projection(block_output) #B,T,V\n",
    "\n",
    "        return vocabulary_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_filename = 'captions.txt'\n",
    "missing = ''\n",
    "\n",
    "with open(caption_filename) as captions:\n",
    "    lines = captions.readlines()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
